# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gCinoxULm1wMFASCvapqP_YCQ7RKMdn_
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/cleaned_goemotions.parquet')
print(df.shape)
df.head()

dist_df = (
    df['main_emotion']
      .value_counts()
      .rename_axis('emotion')
      .reset_index(name='count')
)
dist_df['percent'] = (dist_df['count'] / dist_df['count'].sum() * 100).round(2)
print(dist_df)

df['word_count'] = df['clean_text'].str.split().map(len)
df['char_count'] = df['clean_text'].str.len()

print("\n--- Kelime SayÄ±sÄ± Ä°statistikleri ---")
print(df['word_count'].describe().round(2))

print("\n--- Karakter SayÄ±sÄ± Ä°statistikleri ---")
print(df['char_count'].describe().round(2))

import matplotlib.pyplot as plt


# Histogramlar
fig, axes = plt.subplots(1,2, figsize=(12,4))
axes[0].hist(df['word_count'], bins=30)
axes[0].set_title('Kelime SayÄ±sÄ± DaÄŸÄ±lÄ±mÄ±')
axes[0].set_xlabel('Kelime SayÄ±sÄ±')
axes[0].set_ylabel('Ã–rnek Adedi')

axes[1].hist(df['char_count'], bins=30)
axes[1].set_title('Karakter SayÄ±sÄ± DaÄŸÄ±lÄ±mÄ±')
axes[1].set_xlabel('Karakter SayÄ±sÄ±')
axes[1].set_ylabel('Ã–rnek Adedi')

# 1. BoÅŸ Ã¶rnekleri kaldÄ±r
initial_count = len(df)
df = df[df["word_count"] > 0].reset_index(drop=True)
removed = initial_count - len(df)
print(f"âœ… {removed} adet boÅŸ Ã¶rnek silindi. Kalan Ã¶rnek sayÄ±sÄ±: {len(df)}")

# 2. Yeni kelime/karakter istatistikleri
print("\n--- GÃ¼ncellenmiÅŸ Kelime SayÄ±sÄ± Ä°statistikleri ---")
print(df["word_count"].describe().round(2))

print("\n--- GÃ¼ncellenmiÅŸ Karakter SayÄ±sÄ± Ä°statistikleri ---")
print(df["char_count"].describe().round(2))

# 3. Yeni sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
dist_df = (
    df['main_emotion']
      .value_counts()
      .rename_axis('emotion')
      .reset_index(name='count')
)
dist_df['percent'] = (dist_df['count'] / dist_df['count'].sum() * 100).round(2)
print("\n--- GÃ¼ncellenmiÅŸ Duygu DaÄŸÄ±lÄ±mÄ± ---")
print(dist_df)

# (Opsiyonel) Grafikleri yeniden Ã§izmek istersen
import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))
plt.bar(dist_df['emotion'], dist_df['count'])
plt.xticks(rotation=45, ha='right')
plt.title('GÃ¼ncellenmiÅŸ Duygu SÄ±nÄ±fÄ± DaÄŸÄ±lÄ±mÄ±')
plt.ylabel('Ã–rnek SayÄ±sÄ±')
plt.tight_layout()
plt.show()

p95 = df['word_count'].quantile(0.95)
print("95. persentil kelime sayÄ±sÄ±:", p95)
df = df[df['word_count'] <= p95]

# 1. 95. persentil Ã¼stÃ¼ndekileri filtrele
p95 = df['word_count'].quantile(0.95)
df = df[df['word_count'] <= p95].reset_index(drop=True)
print(f"âœ… {p95} kelime Ã¼stÃ¼ndeki Ã¶rnekler atÄ±ldÄ±. Kalan satÄ±r: {len(df)}")

# 2. Yeni kelime sayÄ±sÄ± Ã¶zetleri
print("\n--- Yeniden Kelime SayÄ±sÄ± Ä°statistikleri ---")
print(df['word_count'].describe().round(2))

# 3. Yeni duygu daÄŸÄ±lÄ±mÄ±
dist_df = (
    df['main_emotion']
      .value_counts()
      .rename_axis('emotion')
      .reset_index(name='count')
)
dist_df['percent'] = (dist_df['count'] / dist_df['count'].sum() * 100).round(2)
print("\n--- Yeniden Duygu DaÄŸÄ±lÄ±mÄ± ---")
print(dist_df)

# 4. (Ä°steÄŸe baÄŸlÄ±) Grafikleri gÃ¼ncelle
import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))
plt.bar(dist_df['emotion'], dist_df['count'])
plt.xticks(rotation=45, ha='right')
plt.title('UÃ§lar AtÄ±ldÄ±ktan Sonra Duygu DaÄŸÄ±lÄ±mÄ±')
plt.ylabel('Ã–rnek SayÄ±sÄ±')
plt.tight_layout()
plt.show()

!pip install datasets

# gcsfsâ€™in istediÄŸi fsspec sÃ¼rÃ¼mÃ¼ne dÃ¶n
!pip install fsspec==2025.3.2

!pip install --upgrade \
  fsspec==2025.3.0 gcsfs==2025.3.0 \
  && pip install --upgrade \
  torch torchvision --upgrade-strategy only-if-needed

import pandas as pd
from datasets import Dataset  # <- Burada Dataset sÄ±nÄ±fÄ±nÄ± alÄ±yoruz

hf_ds = Dataset.from_pandas(
    df[['clean_text','label']].rename(columns={'clean_text':'text'})
).shuffle(seed=42)

from datasets import Dataset, DatasetDict

# hf_ds senin tÃ¼m veri setin
# %80 eÄŸitim, %20 doÄŸrulama
split = hf_ds.train_test_split(test_size=0.2, seed=42)
train_ds = split["train"]
val_ds   = split["test"]

print(f"âœ… Train Ã¶rnek sayÄ±sÄ±: {len(train_ds)}")
print(f"âœ… Val   Ã¶rnek sayÄ±sÄ±: {len(val_ds)}")

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", use_fast=True)

def tokenize_batch(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

# remove_columns ile 'text' sÃ¼tununu atÄ±yoruz, geriye token idâ€™ler kalacak
train_tok = train_ds.map(tokenize_batch, batched=True, batch_size=500, remove_columns=["text"])
val_tok   = val_ds.map(  tokenize_batch, batched=True, batch_size=500, remove_columns=["text"])

print("âœ… Tokenization tamamlandÄ±. Ã–rnek:\n", train_tok[0])

# EÄŸer hÃ¢len pandas DataFrame Ã¼zerinde Ã§alÄ±ÅŸÄ±yorsanÄ±z:
NUM_LABELS = df['label'].nunique()
print("ğŸ”¢ KaÃ§ sÄ±nÄ±f var:", NUM_LABELS)

# Ya da HF Dataset kullanÄ±yorsanÄ±z:
# NUM_LABELS = train_tok.features['label'].num_classes

from transformers import TrainingArguments, IntervalStrategy

# 1) TrainingArguments oluÅŸtur
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    fp16=True,
    logging_dir="./logs",
    report_to="wandb",   # ya da "none" diyebilirsin
)

# 2) DeÄŸerlendirme stratejisini ayarla (epoch sonunda, delay=0 ile)
training_args = training_args.set_evaluate(
    strategy=IntervalStrategy.EPOCH,  # veya "epoch"
    delay=0.0,                        # 0 epoch beklemeden hemen her epoch incele
    batch_size=16,                    # eval batch size
    # steps parametresi sadece strategy="steps" iÃ§in geÃ§erli
)

# 3) Trainerâ€™Ä± baÅŸlat
from transformers import Trainer, AutoModelForSequenceClassification
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=NUM_LABELS
).to(device)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tok,
    eval_dataset=val_tok,
)

# 4) EÄŸitimi baÅŸlat
trainer.train()

from transformers import TrainingArguments, IntervalStrategy, Trainer, AutoModelForSequenceClassification
import torch, os, shutil

# 0) KaÃ§ sÄ±nÄ±f varsa belirle
NUM_LABELS = train_tok.features['label'].num_classes

# 1) TrainingArguments oluÅŸtur
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    fp16=True,
    logging_dir="./logs",
    report_to="wandb",   # veya "none"
)
training_args = training_args.set_evaluate(
    strategy=IntervalStrategy.EPOCH,  # her epoch sonunda validasyon
    delay=0.0,                        # hemen ilk epoch sonunda da Ã§alÄ±ÅŸtÄ±r
    batch_size=16,                    # eval batch size
)

# 2) Model + Trainer
device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=NUM_LABELS
).to(device)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tok,
    eval_dataset=val_tok,
)

# 3) Eski checkpoint klasÃ¶rlerini temizle
output_dir = training_args.output_dir
if os.path.isdir(output_dir):
    removed = 0
    for name in os.listdir(output_dir):
        if name.startswith("checkpoint"):
            shutil.rmtree(os.path.join(output_dir, name))
            removed += 1
    print(f"âœ… {removed} adet eski checkpoint silindi.")
else:
    os.makedirs(output_dir, exist_ok=True)
    print(f"âœ… '{output_dir}' dizini oluÅŸturuldu.")

# 4) EÄŸitimi kaldÄ±ÄŸÄ±n yerden devam ettir
trainer.train(resume_from_checkpoint=True)

!pip install -q --upgrade transformers